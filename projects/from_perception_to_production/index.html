<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model | Marvin Lavechin</title> <meta name="author" content="Marvin Lavechin"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/think.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://marvinlvn.github.io/projects/from_perception_to_production/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Marvin </span>Lavechin</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model</h1> <p class="post-description"></p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_perception_to_production/model-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_perception_to_production/model-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_perception_to_production/model-1400.webp"></source> <img src="/assets/img/from_perception_to_production/model.png" class="img-fluid rounded" width="auto" height="auto" title="Overview of the model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Overview of our speech imitation model.</div> <p>How do infants learn to map variable speech sounds to articulatory movements without instruction? Our self-supervised model tackles this challenge by learning to imitate speech through minimizing distance between input and output speech representations.</p> <p>We found that intermediate layers of wav2vec 2.0 provide optimal speech representations—balancing phonetic information with speaker invariance. This enables learning human-like articulatory patterns and producing intelligible speech across multiple speakers.</p> <p>Here are some examples of our model’s vocal imitation capabilities across the different settings studied in the paper.</p> <p><strong>Paper:</strong> <a href="https://arxiv.org/html/2509.05849v1" target="_blank" rel="noopener noreferrer">https://arxiv.org/html/2509.05849v1</a></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lavechin2025perception</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Lavechin, Marvin and Hueber, Thomas}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="single-speaker-training--single-speaker-testing">Single speaker training / Single speaker testing</h2> <p>In this section, we show examples of audio when the model is trained and evaluated on a single speaker (PB2009, the same speaker used to train the artificial vocal tract). We compare two different models:</p> <ul> <li>One imitating in the Mel-Frequency Cepstral Coefficients (MFCC) space</li> <li>One imitating in the representational space defined by the 7th layer of wav2vec 2.0 (which was found to work best)</li> </ul> <table class="audio-comparison-table"> <thead> <tr> <th rowspan="2">Sentence</th> <th rowspan="2">Input speech</th> <th colspan="2">Speech imitated in the:</th> </tr> <tr> <th>MFCC space</th> <th>Wav2Vec 2.0 layer 7 space</th> </tr> </thead> <tbody> <tr> <td><strong>1</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/pb_sentence2.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/pb2009/pb2009_mfcc_delta_delta2_cosine_seed_0/pb_sentence2.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/pb2009/pb2009_layer_7_cosine_seed_0/pb_sentence2.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> </tr> <tr> <td><strong>2</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/pb_sentence1.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/pb2009/pb2009_mfcc_delta_delta2_cosine_seed_0/pb_sentence1.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/pb2009/pb2009_layer_7_cosine_seed_0/pb_sentence1.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> </tr> </tbody> </table> <p>Both models seem to do quite well in the single-speaker setting, but what happens when training on a more naturalistic training set (read speech + different speakers from the one used to train the artificial vocal tract)?</p> <h2 id="multi-speaker-training--single-speaker-testing">Multi speaker training / Single speaker testing</h2> <p>Here, we train our imitation models on Audiocite (100 hours of read speech produced by 8 speakers). Again, the model is trained to imitate speech either in the MFCC space, or the space defined by wav2vec 2.0’s 7th layer. We test them on PB2009 (also used to train the artificial vocal tract).</p> <table class="audio-comparison-table"> <thead> <tr> <th rowspan="2">Sentence</th> <th rowspan="2">Input speech</th> <th colspan="2">Speech imitated in the:</th> </tr> <tr> <th>MFCC space</th> <th>Wav2Vec 2.0 layer 7 space</th> </tr> </thead> <tbody> <tr> <td><strong>1</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/pb_sentence2.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_mfcc_delta_delta2_cosine_seed_0/pb_sentence2.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_layer_7_cosine_seed_0/pb_sentence2.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> </tr> <tr> <td><strong>2</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/pb_sentence1.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_mfcc_delta_delta2_cosine_seed_0/pb_sentence1.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_layer_7_cosine_seed_0/pb_sentence1.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> </tr> </tbody> </table> <p>Here, we observe that the model trained to imitate our 8 speakers’ speech in the MFCC space completely fails: it cannot imitate the speech of an unknown speaker in an intelligible manner. The model imitating in the wav2vec 2.0’s 7th layer seems to do better and is completely intelligible.</p> <h2 id="multi-speaker-training--multi-speaker-testing">Multi speaker training / multi speaker testing</h2> <p>To make the task even more challenging, we train our model using only 8 speakers from the Audiocite dataset, but then test it on completely different speakers that the model has never encountered during training. This creates a speaker-independent evaluation where the test speakers are entirely unseen by the model throughout the entire training process. We will only consider the model imitating in the wav2vec 2.0’s 7th layer space since the model using MFCCs failed previously.</p> <table class="audio-comparison-table"> <thead> <tr> <th>Sentence</th> <th>Input speech</th> <th>Speech imitated in the Wav2Vec 2.0 layer 7 space</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td><strong>1</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/male_audiobook.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_layer_7_cosine_seed_0/M6_1_s_48.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> Same gender as the vocal tract </td> </tr> <tr> <td><strong>2</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/quebec_french.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_layer_7_cosine_seed_0/M7_2_s_42.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> Quebecois accent + background noise </td> </tr> <tr> <td><strong>3</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/female_audiobook_reverb.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_layer_7_cosine_seed_0/F6_3_s_38.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> Different gender + reverb </td> </tr> <tr> <td><strong>4</strong></td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/original/female_audiobook_music.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> <audio controls=""> <source src="/assets/audio/from_perception_to_production_EMNLP_2025/multi_speaker/8_speakers_6000_mn/8_speakers_6000_mn_layer_7_cosine_seed_0/F4_2_s_0.wav" type="audio/wav"></source> Your browser does not support the audio element. </audio> </td> <td> Different gender + music </td> </tr> </tbody> </table> <p>Across all 4 examples, our imitation model is quite intelligible. Interestingly, the model seems to remove background noise (sentence 2), reverberation artifacts (sentence 3), and music (sentence 4) as those are acoustic feats that cannot easily be generated by the artificial vocal tract.</p> <style>.audio-comparison-table,.audio-examples-table,.audio-layers-table{width:100%;border-collapse:collapse;margin:2rem 0;background-color:var(--global-card-bg-color)}.audio-comparison-table th,.audio-examples-table th,.audio-layers-table th,.audio-comparison-table td,.audio-examples-table td,.audio-layers-table td{border:1px solid var(--global-divider-color);padding:12px;text-align:center;vertical-align:middle;color:var(--global-text-color)}.audio-comparison-table th,.audio-examples-table th,.audio-layers-table th{background-color:var(--global-theme-color);color:var(--global-card-bg-color);font-weight:bold}.audio-comparison-table audio,.audio-examples-table audio,.audio-layers-table audio{width:100%;max-width:300px}.audio-comparison-table tr:nth-child(even),.audio-examples-table tr:nth-child(even),.audio-layers-table tr:nth-child(even){background-color:var(--global-bg-color)}.audio-comparison-table tr:nth-child(odd),.audio-examples-table tr:nth-child(odd),.audio-layers-table tr:nth-child(odd){background-color:var(--global-card-bg-color)}.img-fluid.rounded{background-color:white;padding:10px;border-radius:8px}</style> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0" align="center"> © Copyright 2025 Marvin Lavechin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>